\chapter {Introduction}
 
Convolutional Neural Networks (CNNs) have been adopted as the de-facto method for image processing. However, their high computational complexity and memory usage imposes a burden when operating in embedded environments. Standard embedded platforms lack sufficient GOP/s for practical CNN applications [37]. In recent years, many FPGA implementations of CNNs have been explored to exploit the parallel nature of convolution to obtain satisfactory performance that can support real time inference applications [13]. Most research is focused on convolution computation units (CCUs) as they account for nearly 90\% of compute time and energy usage. Kernel sizes, strides, other layer parameters, and FPGA resources play an important role in the design of convolution computation units (CCUs). For example, [12] implements a CCU specifically for 3x3 kernels with a stride of one using Winograd convolution. Many CNN-FPGA implementations only consider a limited number of CNNs and FPGAs [7, 13, 14, 15, 16], while only a few target compatibilities among diverse CNNs and/or FPGAs [8, 12, 31].

We propose constructing a composer capable of identifying optimizations that can maximize throughput based on a CNN’s architecture and available FPGA resources. We will analyze existing designs including CCUs and data paths found in FPGA-based CNN accelerators and implement various techniques. Performance models of each technique’s throughput and resource usage against different convolutional layer configurations will drive the composer’s choice in selecting optimal designs. Additionally, we will wrap the composer within Keras Synthesized, a framework that generates high-level synthesis (HLS) code for CNN-FGPA implementations based on the composer’s findings. We will then evaluate these implementations using various FPGAs and CNNs that represent diversity in resources and parameters, respectively, and compare to related works.