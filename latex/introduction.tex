\chapter {Introduction}
 
Convolutional Neural Networks (CNNs) have been adopted as the de-facto method for image processing. However, their high computational complexity and memory usage imposes a burden when operating in embedded environments. Standard embedded platforms lack sufficient GOP/s for practical CNN applications \cite{guo2018angel}. In recent years, many FPGA implementations of CNNs have been explored to exploit the parallel nature of convolution to obtain satisfactory performance that can support real time inference applications \cite{toledo2012fpga}. Most research is focused on convolution computation units (CCUs) as they account for nearly 90\% of compute time and energy usage. Kernel sizes, strides, other layer parameters, and FPGA resources play an important role in the design of CCUs. Many CNN-FPGA implementations only consider hard-coded CCUs for specific CNN architectures, allowing granular optimizations to fully maximize performance \cite{liu2016automatic, toledo2012fpga, hwang2017efficient, bettoni2017convolutional, jiao2017accelerating}. Only a few consider reconfigurable designs which allow various CNN compatibility without having to recompile\cite{tu2017deep, dicecco2016caffeinated, lu2017evaluating}.

Recompiling an FPGA has significant implications when using one as an edge device. The place and route stage maps HDL onto the FPGA itself, and routes the connections between components. This stage alone can take multiple hours. When considering a CNN-FPGA application that is performing mission-critical computations, that much downtime for an update can hinder its usage altogether. Thus, we consider reconfigurable designs that allow compatibility for various layers found in a CNN. This allows a CNN-FPGA device to be updated with a new model by only updating its weights and layer configuration stored in the external memory of the FPGA. If extended downtime is permissible, a reconfigurable design on an FPGA can update with the latest breakthrough methods, unlike an ASIC device. The penalty of recompilation would only be necessary for modifications to the layer compatibility itself.

In this work, we implement a reconfigurable CCU using the Intel HLS Compiler and benchmark it using the layer sizes from AlexNet. Our results shows weak performance, but a glimpse into how machine learning scientists can leverage HLS to build flexible, edge compatibile CNN devices while abstracting out the many intricate details of hardware.

%We propose constructing a composer capable of identifying optimizations that can maximize throughput based on a CNN’s architecture and available FPGA resources. We will analyze existing designs including CCUs and data paths found in FPGA-based CNN accelerators and implement various techniques. Performance models of each technique’s throughput and resource usage against different convolutional layer configurations will drive the composer’s choice in selecting optimal designs. Additionally, we will wrap the composer within Keras Synthesized, a framework that generates high-level synthesis (HLS) code for CNN-FGPA implementations based on the composer’s findings. We will then evaluate these implementations using various FPGAs and CNNs that represent diversity in resources and parameters, respectively, and compare to related works.