\chapter{Background}
\section{Field Programmable Gate Arrays}
FPGAs are integrated circuits designed to be reconfigurable. They are made up of programmable logic blocks and interconnects that allow the designer to create programs made up of modular blocks. For our work, we are interested in the amounts of digital signal processors (DSPs), logic elements (LEs), block RAM (BRAM), and dynamic RAM (DRAM) equipped in FPGAs. DSPs are specialized microprocessor designed to perform primitive signal processing operations including multiply-accumulate (MAC). Given enough DSPs, they can be used to accelerate embarrassingly parallel tasks including matrix-multiply, convolution, and Fast Fourier transform (FFT) much faster than a traditional processer. LEs are resources on an FPGA that can perform logic functions, including logic gates, registers, and multiplexers. BRAM is limited on-chip random access memory. Lastly, DRAM is off-chip random access memory capable of storing much more data than on-chip at the cost of access latency. It is imperative to access DRAM such that data bursts. Burst mode refers to sending data repeatedly without performing any additional steps including indexing. DRAM implements burst mode by automatically fetching the next memory contents before they are requested. Thus, if memory is accessed in a contiguous manner, the number if memory access requests are minimized. Quantization is the process of constraining floating points to a discrete set of bits for precision. The radix point refers to the separation of the exponent and mantissa of a floating point. It can be placed anywhere relative to the significant digits of a number. FPGAs are often equipped with DSPs that support variable precisions that scale with respect to precision.

\section{Convolutional Neural Networks}
CNNs are a class of feed-forward artificial neural networks that excel in image classification. Notably because their shared, trained weights are able to identify patterns irrespective of their location within input images. Convolutional kernels, which contain these weights, stride along the axes and perform convolution to map inputs to feature maps, where it is easier to identify desired patterns to classify. Performing convolution and other operations on feature maps is referred to as a layer. Layers are stacked together, forming a CNN architecture that have the ability to classify 1000 objects with over 80\% accuracy given an input image \cite{krizhevsky2012imagenet}.

\section{High Level Synthesis}
HLS is a compiler that interprets an algorithm and compiles it into a digital hardware representation. Its purpose is to design hardware at a much faster pace. Similar to high level languages, HLS limits users from accessing low-level constructs, which can potentially hinder performance for algorithms that require meticulous designs. The Intel HLS compiler claims it generates high-quality code that is orders of magnitude faster than register-transfer level (RTL), requires 80\% less lines of code, meets performance, and is within 10-15\% of the area of hand- coded RTL. This is accomplished using existing logic units (IPs); the intellectual property of a party.

In many cases, requiring the FPGA on-hand is unfeasible for testing code. ModelSim is a tool that allows you to simulate RTL within software. Our work will use both the Intel HLS Compiler for faster development and more performant code compared to writing our own RTL. Additionally, we will use ModelSim to conduct experiments that analyze performance on a variety of FPGA architectures.

\section{Benchmarking}
Giga operations per second (GOP/s) is a standard metric for analyzing accelerator performance. It is simpler than runtime because it does not rely on experiments using the same inputs for a comparison between two accelerators, and is hardware agnostic. Floating point operations per second (FLOP/s) is not considered because it abstracts the complexities of primitive operations including divide, multiply, add, etc. With FPGAs targeting granular acceleration techniques, it is best not to abstract any performance characterizations. GOPs per watt (GOP/W) considers performance with respect to energy usage. It is an important metric to consider for minimizing long term energy costs and efficiency.

\section{Layers}
Table I shows primitive operations found within the popular CNN architectures in Table II. We classify each layer into a compute-unit (CU) that represent modular components in a FPGA pipeline. For example, it is typical for ReLU activations (ACT) and batch normalization (NORM) to be adjacent to each other in computation flow. We classify this as the ACTN compute-unit. Alter (ALT) layers modify tensors in some form that changes their dimensionality. It is ideal to structure memory writes from previous, adjacent layers from ALT layers to form the transformed tensors in memory. This can alleviate resource usage from ALT CUs.

For deep networks, convolution computation units (CCUs) account for nearly 90\% of computation and energy consumption. It is crucial for CCUs to reach peak performance when executing. Other CUs should minimize DSP usage and area occupancy to dedicate most resources for CCUs, while maintaining a pipelined computational flow to hide latency and increase performance.
