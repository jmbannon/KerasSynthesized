\chapter{Background}
\section{Field Programmable Gate Arrays}
FPGAs are integrated circuits designed to be reconfigurable. They are made up of programmable logic blocks and interconnects that allow the designer to create programs made up of modular blocks. For our work, we are interested in the amounts of digital signal processors (DSPs), logic elements (LEs), block RAM (BRAM), and dynamic RAM (DRAM) equipped in FPGAs. DSPs are specialized microprocessor designed to perform primitive signal processing operations including multiply-accumulate (MAC). Given enough DSPs, they can be used to accelerate embarrassingly parallel tasks including matrix-multiply, convolution, and Fast Fourier transform (FFT) much faster than a traditional processer. LEs are resources on an FPGA that can perform logic functions, including logic gates, registers, and multiplexers. BRAM is limited on-chip random access memory. Lastly, DRAM is off-chip random access memory capable of storing much more data than on-chip at the cost of access latency. It is imperative to access DRAM such that data bursts. Burst mode refers to sending data repeatedly without performing any additional steps including indexing. DRAM implements burst mode by automatically fetching the next memory contents before they are requested. Thus, if memory is accessed in a contiguous manner, the number if memory access requests are minimized. Quantization is the process of constraining floating points to a discrete set of bits for precision. The radix point refers to the separation of the exponent and mantissa of a floating point. It can be placed anywhere relative to the significant digits of a number. FPGAs are often equipped with DSPs that support variable precisions that scale with respect to precision.

\section{Convolutional Neural Networks}
CNNs are a class of feed-forward artificial neural networks that excel in image classification. Notably because their shared, trained weights are able to identify patterns irrespective of their location within input images. Convolutional kernels, which contain these weights, stride along the axes and perform convolution to map inputs into feature maps, where it is easier to identify desired patterns to classify. CNN architectures are comprised of many different types of layers, as shown in \ref{layer_types}. Our work is focused only on convolution and padding as they account for nearly 90\% of computation and energy consumption (NEED REF).

\begin{figure}
	\centering
	\begin{tabular}{ |p{4cm}|p{11.6cm}|  }
		\hline
		Name & Description \\
		\hline
		Activation & Applies a monotonic function on all input values\\
		Add & Takes same sized parallel tensors and adds them together by value \\
		AveragePooling2D & Strides along feature maps with k-by-k kernel and outputs average of all elements \\
		BatchNormalization & Normalizes all inputs to be between 0 and 1 \\
		Concatenate & Concatenates two tensors along some axis \\
		Conv2D & Performs k convolutions on a 3d tensor depth-wise \\
		Cropping2D & Consider only a rectangular subset of an input feature map, disregard the rest \\
		Dense & Multiplies elements in input vector by n weights \\
		Dropout & Multiplies n random elements by 0 \\
		Flatten & Flattens a tensor to a 1 dimensional vector, used to feed to dense layers \\
		MaxPooling2D & Reduces an output feature map by striding across each channel taking the max of k-by-k values \\
		\hline
	\end{tabular}
	\label{layer_types}
	\caption{Common CNN Layers}
\end{figure}

\section{High Level Synthesis}
HLS is a compiler that interprets an algorithm and compiles it into a digital hardware representation. Its purpose is to design hardware at a much faster pace. Similar to high level languages, HLS limits users from accessing low-level constructs, which can potentially hinder performance for algorithms that require meticulous designs. The Intel HLS compiler claims it generates high-quality code that is orders of magnitude faster than register-transfer level (RTL), requires 80\% less lines of code, meets performance, and is within 10-15\% of the area of hand- coded RTL. This is accomplished using existing logic units (IPs); the intellectual property of a party.

In many cases, requiring the FPGA on-hand is unfeasible for testing code. ModelSim is a tool that allows you to simulate RTL within software. Our work will use both the Intel HLS Compiler for faster development and more performant code compared to writing our own RTL. Additionally, we will use ModelSim to conduct experiments that analyze performance on a variety of FPGA architectures.

\section{Benchmarking}
Giga operations per second (GOP/s) is a standard metric for analyzing accelerator performance. It is simpler than runtime because it does not rely on experiments using the same inputs for a comparison between two accelerators, and is hardware agnostic. Floating point operations per second (FLOP/s) is not considered because it abstracts the complexities of primitive operations including divide, multiply, add, etc. With FPGAs targeting granular acceleration techniques, it is best not to abstract any performance characterizations. GOPs per watt (GOP/W) considers performance with respect to energy usage. It is an important metric to consider for minimizing long term energy costs and efficiency.
