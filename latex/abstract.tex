\abstract{%
Deep learning continues to be the revolutionary method used in pattern recognition applications including image, video, and speech processing. Convolutional Neural Networks (CNNs) in particular have outperformed every competitor in image classification benchmarks, but suffer from high computation and storage complexities. It is becoming more apparent to extend this breakthrough technology to embedded applications that demand low power and mission critical response times. Consequently, embedded CNNs deployed on the edge require compact platforms capable of accelerated computing. Previous works have explored methods to optimize convolution computation within Field Programmable Gate Arrays (FPGAs). Many of which only consider supporting a single CNN architecture. While this approach allows precise optimizations structured around a specific CNN, it restricts the FPGA from updating its model without tremendous compile times upwards to hours. For applications on the edge, this much downtime can be a deal breaker. In this work, we explore state-of-the-art reconfigurable CNN-FPGA architectures and implement our own using the Intel High-Level-Synthesis (HLS) Compiler which allows arbitrary input sizes. While our results yield poor performance, we show how the time-to-code and implications of a better optimized reconfigurable CNN-FPGA architecture can make a significant impact on deployments of CNNs in edge environments.
}